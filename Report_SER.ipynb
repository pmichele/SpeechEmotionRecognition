{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not spend too much time trying to get very tiny metrics improvement. Once you have a model with a correct predictive power, you should better spend time explaining your data cleaning & preparation pipeline as well as explanations & visualizations of the results.\n",
    "\n",
    "The goal is to see your fit with our company culture & engineering needs, spending 50h on an over-complicated approach will not give you bonus points compared to a simple, yet effective, to-the-point solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will be working with is called Emo-DB and can be found [here](http://emodb.bilderbar.info/index-1280.html).\n",
    "\n",
    "It is a database containing samples of emotional speech in German. It contains samples labeled with one of 7 different emotions: Anger, Boredom, Disgust, Fear, Happiness, Sadness and Neutral. \n",
    "\n",
    "Please download the full database and refer to the documentation to understand how the samples are labeled (see \"Additional information\")\n",
    "   \n",
    "The goal of this project is to develop a model which is able to **classify samples of emotional speech**. Feel free to use any available library you would need, but beware of re-using someone else's code without mentionning it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end-goal is to deliver us a zip file containing:\n",
    "* This report filled with your approach, in the form of an **iPython Notebook**.\n",
    "* A **5-10 slides PDF file**, containing a technical presentation covering the important aspects of your work\n",
    "* A Dockerfile which defines a container for the project. The container should handle everything (download the data, run the code, etc...). When running the container it should expose the jupyter notebook on one port and expose a Flask API on another one. The Flask app contains two endpoints:\n",
    "  - One for training the model\n",
    "  - One for querying the last trained model with an audio file of our choice in the dataset\n",
    "* A README.md which should contain the commands to build and run the docker container, as well as how to perform the queries to the API. \n",
    "* Any necessary .py, .sh or other files needed to run your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My solution follows the approach proposed in this reference paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Dias Issa, M. Fatih Demirci, Adnan Yazici,\n",
    "Speech emotion recognition with deep convolutional neural networks,\n",
    "Biomedical Signal Processing and Control,\n",
    "Volume 59,\n",
    "2020,\n",
    "101894,\n",
    "ISSN 1746-8094,\n",
    "https://doi.org/10.1016/j.bspc.2020.101894."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors propose a CNN-based architecture for emotion recognition. In the paper they claim that their method outperforms all existing methods with the exception of one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jianfeng Zhao, Xia Mao, Lijiang Chen,\n",
    "Speech emotion recognition using deep 1D & 2D CNN LSTM networks,\n",
    "Biomedical Signal Processing and Control,\n",
    "Volume 47,\n",
    "2019,\n",
    "Pages 312-323,\n",
    "ISSN 1746-8094,\n",
    "https://doi.org/10.1016/j.bspc.2018.08.035."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this latter paper is more demanding in terms of work and implementation, I decided that for the scope of this project the difference in performance between the two approaches is negligible (less than 1\\%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa, librosa.display\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename(x):\n",
    "    return os.path.split(x)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to keep the sampling rate as the original of the dataset to avoid upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000 # Expected sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(x):\n",
    "    signal, _sr = librosa.load(x, sr=None)\n",
    "    assert _sr == sr\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "test_dir = \"/data/test\"\n",
    "raw_data_dir = \"/raw_data\"\n",
    "flask_dir = \"/src/flask_app\"\n",
    "models_dir = \"/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation and augmentation is automatically handled at container's startup. This way the the user does not need to run any command other than querying the api and the notebook is entirely optional as it only contains data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all the container translates the german character code for the emotion in the filename into an easily readable english string, for example 'W' is mapped to 'anger'. The filename map is provided as a json file so that the python source code does not need to be inspected/recompiled for modifications of the mapping. This is a general practice I applied several time in this project: parameters are separated from the python code. I chose json format for representing config objects.\n",
    "\n",
    "Note that in the reference paper fear corresponds to anxiety here, as the german word 'Angst' can have both meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the container performs random train/test split, choosing the test size to be 20\\% as this roughly corresponds to the test size used in the reference paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid bias, we never inspect the test set and only show some analysis of train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the test samples\n",
    "test_set = {filename(x) for x in glob(os.path.join(test_dir, \"*/*.wav\"))}\n",
    "x_train = [x for x in glob(os.path.join(raw_data_dir, \"*.wav\")) if filename(x) not in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle for unbiased data exploration\n",
    "random.shuffle(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio samples seem normalized by amplitude. There are several peaks and the amplitude is not stable as we would expect from a complex signal as the human speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    signal = load_sample(x_train[i])\n",
    "    librosa.display.waveplot(signal, sr=sr)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we show the frequency domain. We can observe that most of the energy is concentrated on low frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    signal = load_sample(x_train[i])\n",
    "    fft = np.fft.fft(signal)\n",
    "    magnitude = np.abs(fft)\n",
    "    frequency = np.linspace(0, sr//2, len(magnitude)//2)\n",
    "    plt.plot(frequency, magnitude[:(len(magnitude)//2)])\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the spectrogram doesn't show noisy patterns as speckles or high amplitude concentrated on specific frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    signal = load_sample(x_train[i])\n",
    "    stft = librosa.core.stft(signal)\n",
    "    spectrogram = librosa.amplitude_to_db(np.abs(stft))\n",
    "    librosa.display.specshow(spectrogram, sr=sr)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sort the samples by speaker and text index we observe that the distribution of the sequence length depends on the speaker. Each speaker is assigned a random color to distinguish them and we can observe that some speakers have a preference for quick utterance (such as the 4th) while others prefer to take more time (the second speaker being the slowest). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.title(\"Sequence Lengths per Speaker\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Samples Sorted by Speaker and Text Index\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "color_assign = {}\n",
    "colors = []\n",
    "lengths = []\n",
    "sorted_x_train = sorted(x_train)\n",
    "\n",
    "# assign a random color to each speaker so that\n",
    "# they don't look similar\n",
    "for file in sorted(sorted_x_train):\n",
    "    speaker = filename(file)[:2]\n",
    "    color_assign[speaker] = 0\n",
    "assignments = np.arange(0, len(color_assign))\n",
    "random.shuffle(assignments)\n",
    "for i, key in enumerate(color_assign.keys()):\n",
    "    color_assign[key] = assignments[i]\n",
    "\n",
    "# plot sequence length distribution across speakers\n",
    "for file in sorted_x_train:\n",
    "    signal = load_sample(file)\n",
    "    speaker = filename(file)[:2]\n",
    "    lengths.append(len(signal)/sr)\n",
    "    colors.append(color_assign[speaker])\n",
    "plt.scatter(np.arange(0, len(lengths)), lengths, marker='o', cmap='jet', s=15, c=colors)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, most sequences are between 1 and 6 seconds disregarding outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of the sequence lengths\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.boxplot(lengths)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning data augmentation, I followed a similar approach as the one suggested in the reference paper. It consists of two transformations, namely shifting the audio sample by a small margin and stretching it to speed it up or slowing it down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the shift offset I manually inspected several training samples picked uniformly at random and measured the maximum distance between the begins of utterance. I found 100 ms to be around the longest offset. Thus, I doubled that value and picked 200 ms as the maximum shift. The random shift is then drawn uniformely at random in range [0, 200] ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slowing down or speeding up the audio I used the same parameters as in the reference paper, namely 0.81x and 1.23x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I did not add noise to the samples as in the paper because I already found good performance of the netowrk without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reference paper the authors leverage multiple speech features at once, namely \n",
    "1. MFCC, \n",
    "2. Mel-scaled spectrogram, \n",
    "3. Chromagram, \n",
    "4. Spectral contrast,\n",
    "5. Tonnetz representation. \n",
    "\n",
    "This approach is novel (it has been published this year) and it has shown competitive performance with RNN-based models. Thus, I extracted and concatenated multiple features from a sample for a total of 193 features for the neural network classifier. However, the individual number of features of each is not specified in the paper. Fortunately, I found that tonnetz representation always produces 6 values and a natural number of bands for the spectral contrast is also 6, which results in 7 values. As a result, we are left with 180 values to be generated by 3 features. Hence, I guessed that in the paper they used 60 values per feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below I show one of these features, namely the MFCC coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    signal = load_sample(x_train[i])\n",
    "    mfcc = librosa.feature.mfcc(signal, n_mfcc=60)\n",
    "    librosa.display.specshow(mfcc, sr=sr)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"MFCC\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to batch the sequences for neural network training we have to handle the varying sizes of each sequence. In the reference paper the authors reduce the sequences to the mean of each feature along time. This way, there is no need to handle padding / truncation of the signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected model corresponds to 'Model A' in the reference paper. The authors propose various ensembles in order to boost performance by a few points. However, for the scope of this project the gain in performance is not worth the amount of work and resources required as a model with reasonable performance is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is composed of two units, each of which has several 1-d convolutions of size 5 and ReLUs (plus batchnorm and dropout). Before running the second unit the signal is pooled with a relatively large window (8) so that the receptive field of the second unit is large enough to blend multiple features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagram depicting the nerual network's architecture\n",
    "model = cv2.imread(os.path.join(flask_dir, \"model.jpg\"), cv2.IMREAD_GRAYSCALE)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(model, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyperparameters were set according to the values reported in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starting the training of the neural network we will use the Flask API, which I designed in a modular fashion. Indeed, one can extend the API with more architectures and reuse the same framework for training and testing the model. For the time being only the model just described above is implemented. For starting its training you can query the following endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With successive queries of this same endpoint you can get information about the progress of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scope of this demo, I provided pre-trained model weights so that training is not required for following the analysis of results in this report. Moreover, queries are automatically configured to rely on the pre-computed weights too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that with the given setup the model loss is minimized smoothly without oscillation and that the model converged after around the same number of epochs reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(models_dir, \"best_cnn/log_loss.txt\")) as loss_file:\n",
    "    loss = np.array([float(x) for x in loss_file.read().split('\\n') if x])\n",
    "plt.title(\"Loss per test iteration\")\n",
    "plt.grid(True)\n",
    "plt.xlabel('Test Iteration')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analysis of results, we can query the Flask API for single items with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl \"http://localhost:5000/predict?sample=<some_sample>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain the list of test samples with this other command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl \"http://localhost:5000/predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl \"http://localhost:5000/predict?sample=16a07Fb.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or I prepared a pickle containing the predictions of all samples. This is the same as querying every utterance in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_pickle(os.path.join(models_dir, \"best_cnn/test/predictions.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the loss monotonically decreases, I observed that the accuracy reached a plateau after a number of test iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(models_dir, \"best_cnn/log_accuracy.txt\")) as acc_file:\n",
    "    acc = []\n",
    "    for line in acc_file.read().split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        acc.append(float(line.split(\",\")[0]))\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Accuracy per test iteration\")\n",
    "plt.grid(True)\n",
    "plt.yticks(np.arange(0,1,0.025))\n",
    "plt.xlabel('Test Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observed that after 300 epochs the max accuracy is around 0.90. This is surprising as the accuracy reported in the paper is 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max accuracy\n",
    "(predictions.Prediction == predictions.Truth).sum() / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After further inspection of the reference paper I found that the accuracy reported for emo-db among various publications in the literature is inconsistent as some researchers discarded 15 samples from the dataset. The authors show that by adopting that subset they managed to obtain around 0.95 accuracy with their best model (which is an ensemble of 7 binary classifiers). The simple cnn that I implemented does not have reported accuracy in their paper for the subset of emo-db. However, in the full dataset it has slightly lower accuracy compared to the ensemble of 7 models according to the authors. Thus it's probable that the cnn would have 0.90 accuracy in the subset of emo-db "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test set is relatively small (around 100 samples), these 15 diffcult examples can have a large impact. Indeed, in the worst case all of them could be in the test set, which could result in a 0.15 decrease of performance compared to the case where they are all in the training set. In conclusion the measured accuracy has high variance depending on the split and this could be a reasonable explanation for the 0.08 variation that I observed. Arguably, implementing a 5-fold cross-validation as in the reference paper might reduce the issue, as the reported accuracy would be an average of 5 different splits and variance decreases with averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix below shows that anxiety and disgust are the hardest emotions to recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((7, 7))\n",
    "for i in range(7):\n",
    "    i_truth = predictions[predictions.Truth == i]\n",
    "    i_predictions = i_truth.groupby('Prediction').size()\n",
    "    for j in i_predictions.index:\n",
    "        matrix[i, j] = i_predictions[j] / len(i_truth)\n",
    "    \n",
    "classes = ['anger', 'anxiety', 'boredom', 'disgust', 'happiness', 'neutral', 'sadness']\n",
    "confusion_matrix = pd.DataFrame(matrix, index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93% of the predictions have more than 50% confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_confidence = predictions[predictions.Confidence > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(high_confidence) / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However removing low confidence predictions does not improve accuracy significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Max accuracy for high confidence\n",
    "(high_confidence.Prediction == high_confidence.Truth).sum() / len(high_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model has 3\\% better accuracy for female speakers compared to male speakers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fields inferring them from the file name\n",
    "predictions['Speaker'] = [int(x[:2]) for x in predictions.index]\n",
    "predictions['Text'] = [x[2:5] for x in predictions.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_speakers = {8, 9, 13, 14, 16}\n",
    "predictions_on_female = predictions[predictions.apply(lambda x: x.Speaker in female_speakers, axis=1)]\n",
    "# Max accuracy for female speakers\n",
    "(predictions_on_female.Prediction == predictions_on_female.Truth).sum() / len(predictions_on_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_male = predictions[predictions.apply(lambda x: x.Speaker not in female_speakers, axis=1)]\n",
    "# Max accuracy for male speakers\n",
    "(predictions_on_male.Prediction == predictions_on_male.Truth).sum() / len(predictions_on_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is performance by utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_by_text = predictions.groupby('Text').apply(lambda df: len(df[df.Prediction == df.Truth]) / len(df))\n",
    "plt.bar(performance_by_text.index, performance_by_text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a weak correlation between sequence length and accuracy of prediction. It seems that the model performs slightly better if the sequence is longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = []\n",
    "for file in sorted(glob(os.path.join(test_dir, \"*/*.wav\"))):\n",
    "    signal = load_sample(file)\n",
    "    seq_lengths.append(len(signal) / sr)\n",
    "equals = predictions.Prediction == predictions.Truth\n",
    "equals = equals.sort_index()\n",
    "equals = equals.reset_index()\n",
    "equals = equals[0] # convert equals to series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals.corr(pd.Series(seq_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can correlate the sentence complexity to accuracy. We can estimate complexity as the time required to utter the sentence. We compute this by reducing the sequence lengths for a given text to the median along the speakers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextLength(df):\n",
    "    \"\"\"Returns the length of the sentence estimated by the median of the sequence lengths along all speakers\"\"\"\n",
    "    seq = []\n",
    "    for i, row in df.iterrows():\n",
    "        signal = load_sample(os.path.join(raw_data_dir, i))\n",
    "        time = len(signal) / sr\n",
    "        seq.append(time)\n",
    "    return np.median(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = predictions.groupby('Text').apply(getTextLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, performance is negatively affected by sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_by_text.corr(sentence_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
